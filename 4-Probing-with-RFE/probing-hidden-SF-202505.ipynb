{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea4f7e5-eed7-44b8-9079-16b82027d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook to (i) extract L17 & L21 hidden dims for the last token on Llama3 model with SF/NP-v-control data\n",
    "#             (ii) run logistic regression models on this data\n",
    "\n",
    "# pip install nnsight transformers torch scikit-learn seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf22079-db1a-4675-91e2-35045c8f8405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5871968ec6d446f8a68643e6dfd0de79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb1a6bd348243368d41ac5105227e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ba55f2f3c3418687c45a216d82ef69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d51860f5a34dd9b8f8cab6852b2dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d1d5954b0e40f8b80849b2d2167cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ba9515217a477baac5ed09c3e86ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294821bb8bde464b974f3672f5f1f3d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1e577d1b094fd2836babe5683dc2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e851bc81844e8993b7fd6f71b53446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load llama3\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "\n",
    "XT = \"hf_your_token_here\"  # replace with your Hugging Face token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", token=XT)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "                                             token=XT, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6986026-48fc-4883-8074-aa3907b0f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the output of Layer X\n",
    "def get_hidden_LT(texts, model, tokenizer, layer=17, token_pos=-1):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    # Extract the hidden dims for layer  & token_pos  (-1 == last)\n",
    "    hidden_states = outputs.hidden_states[layer]\n",
    "    hidden_output = hidden_states[:, token_pos :].squeeze().cpu().numpy()\n",
    "    return hidden_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab40ae6-dde7-4d39-9706-a55293b2a8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_label\n",
      "nurturing-parent    39\n",
      "control             38\n",
      "strict-father       38\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "import pandas as pd\n",
    "df_ = pd.read_csv(\"../1-Generation-and-Dataset/data-stories-for-probing-20240525.csv\")\n",
    "print(df_.frame_label.value_counts())\n",
    "\n",
    "df1 = df_[df_.frame_label!='nurturing-parent']  # df for: SF-v-control\n",
    "df2 = df_[df_.frame_label!='strict-father']  # df for: NP-v-control\n",
    "labels1 = np.array(df1.frame_label.map({'strict-father': 1, 'nurturing-parent':1, 'control': 0}))  \n",
    "labels2 = np.array(df2.frame_label.map({'strict-father': 1, 'nurturing-parent':1, 'control': 0}))  \n",
    "texts1 = list(df1.story)\n",
    "texts2 = list(df2.story)\n",
    "#print(df_.story.apply(lambda x: len(x.split())).max())  # longest text is 213 words  ≈ 425 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "683ea4e5-73c8-4314-bb93-6feaf14b14a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 (4096,)\n",
      "77 (4096,)\n"
     ]
    }
   ],
   "source": [
    "# Extract features for each text\n",
    "features1 = []\n",
    "for text in texts1:\n",
    "    features1.append(get_hidden_LT(text, model, tokenizer))\n",
    "    #torch.cuda.empty_cache()  # Clear the CUDA cache\n",
    "\n",
    "features2 = []\n",
    "for text in texts2:\n",
    "    features2.append(get_hidden_LT(text, model, tokenizer))\n",
    "    #torch.cuda.empty_cache()  # Clear the CUDA cache\n",
    "\n",
    "print(len(features1), features1[0].shape)  # 76 (4096,)\n",
    "print(len(features2), features2[0].shape)  # 77 (4096,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "084bd94d-9fee-4b91-bdc6-1135de673b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export as Pickle\n",
    "import pickle\n",
    "# with open('hidden-sf-17.pkl', 'wb') as p:\n",
    "#     dat = {'features': features1, 'labels': labels1, 'info':'SFvctrl @L17'}\n",
    "#     pickle.dump(dat, p)\n",
    "\n",
    "# with open('hidden-np-17.pkl', 'wb') as p:\n",
    "#     dat = {'features': features2, 'labels': labels2, 'info':'NPvctrl @L17'}\n",
    "#     pickle.dump(dat, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45cc9e38-44bd-4b91-8f28-c0a0892f8403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying SF\n",
      "going for rfe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, now clf\n",
      "Test/Train Accuracy≈F1 0.9375 0.9333333333333333\n",
      "Selected Feats: [ 133 1292 2040 2692 3261]\n"
     ]
    }
   ],
   "source": [
    "# CLASSIFY SF\n",
    "print(\"Classifying SF\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1 = np.array(features1)\n",
    "y1 = np.array(labels1)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize RFE with the logistic regression model to select top N features\n",
    "clf1 = LogisticRegression(solver='liblinear')  # a faster solver (?)\n",
    "rfe1 = RFE(estimator=clf1, n_features_to_select=5)\n",
    "print(\"going for rfe\")\n",
    "rfe1.fit(X_train1, y_train1)\n",
    "print(\"done, now clf\")\n",
    "\n",
    "# Transform the training and testing data to the selected features and train logistic model on these\n",
    "X_train1_rfe = rfe1.transform(X_train1)\n",
    "X_test1_rfe = rfe1.transform(X_test1)\n",
    "clf1.fit(X_train1_rfe, y_train1)\n",
    "\n",
    "# Make predictions & evaluate\n",
    "y_test_pred1 = clf1.predict(X_test1_rfe)\n",
    "y_train_pred1 = clf1.predict(X_train1_rfe)\n",
    "\n",
    "print(\"Test/Train Accuracy≈F1\", accuracy_score(y_test1, y_test_pred1), accuracy_score(y_train1, y_train_pred1)) \n",
    "#print(classification_report(y_test1, y_pred1))\n",
    "print(\"Selected Feats:\",  np.where(rfe1.support_)[0])  # also rfe1.ranking_\n",
    "\n",
    "# WOW 0.94 => RFE-10: lowers it to 0.88, not bad (same as RFE-100), RFE5= 0.94/0.93, RFE3=0.88/0.85, RFE1=0.81/0.78\n",
    "# Selected Feats: [ 133** 1292 2040* 2692*** 3261]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "591205f7-aef4-46d8-9a04-f3f933295132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying NP\n",
      "going for rfe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, now clf\n",
      "Test/Train Accuracy≈F1 0.875 0.9016393442622951\n",
      "Selected Feats: [ 529  540 1658 1707 2209]\n"
     ]
    }
   ],
   "source": [
    "# CLASSIFY NP\n",
    "print(\"Classifying NP\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X2 = np.array(features2)\n",
    "y2 = np.array(labels2)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "clf2 = LogisticRegression(solver='liblinear')\n",
    "rfe2 = RFE(estimator=clf2, n_features_to_select=5)\n",
    "print(\"going for rfe\")\n",
    "rfe2.fit(X_train2, y_train2)\n",
    "print(\"done, now clf\")\n",
    "\n",
    "# Transform the training and testing data to the selected features and train logistic model on these\n",
    "X_train2_rfe = rfe2.transform(X_train2)\n",
    "X_test2_rfe = rfe2.transform(X_test2)\n",
    "clf2.fit(X_train2_rfe, y_train2)\n",
    "\n",
    "# Evaluate\n",
    "y_testpred2 = clf2.predict(X_test2_rfe)\n",
    "y_trainpred2 = clf2.predict(X_train2_rfe)\n",
    "\n",
    "print(\"Test/Train Accuracy≈F1\", accuracy_score(y_test2, y_testpred2), accuracy_score(y_train2, y_trainpred2))  \n",
    "#print(classification_report(y_test2, y_pred2))\n",
    "print(\"Selected Feats:\", np.where(rfe2.support_)[0])  \n",
    "\n",
    "# => RFE40=0.94!, RFE30=0.88, RFE20=0.94!, RFE10=0.88, RFE5=0.88/???, RFE2=0.94/0.86, RF1=0.88/0.80\n",
    "# Selected Feats:  [ 529*** 540 1658** 1707 2209]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "594a170c-c641-4fe0-9ae6-73fe0a3ddbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Extract the coefficients (beta values)\n",
    "# coefficients1 = clf1.coef_[0]\n",
    "\n",
    "# # Plot the coefficients to see their magnitude\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.bar(range(len(coefficients1)), coefficients1)\n",
    "# plt.xlabel('Feature Index')\n",
    "# plt.ylabel('Coefficient Value')\n",
    "# plt.title('Feature Importance (Coefficient Values)')\n",
    "# plt.show()\n",
    "\n",
    "# # Identify the most important features (largest absolute coefficients)\n",
    "# important_features = np.argsort(np.abs(coefficients1))[::-1]\n",
    "# print(\"Top 10 most important features (by absolute value of coefficients):\")\n",
    "# print(important_features[:10])\n",
    "# print(\"Corresponding coefficient values:\")\n",
    "# print(coefficients1[important_features[:10]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
