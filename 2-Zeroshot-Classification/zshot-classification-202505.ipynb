{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2 shape:  <bound method NDFrame.head of        ID    Model Source                                        Frame  \\\n",
      "0       0   llama2   orig                                strict-father   \n",
      "1       2   llama2  scifi                                strict-father   \n",
      "2       3   llama2   orig                             nurturing-parent   \n",
      "3       5   llama2  scifi                             nurturing-parent   \n",
      "4       6   llama2   orig                                  us-vs.-them   \n",
      "..    ...      ...    ...                                          ...   \n",
      "265  1000   vicuna   orig             information-spreads-like-a-virus   \n",
      "266  1001  mistral   orig             information-spreads-like-a-virus   \n",
      "267  1002   vicuna  bible                             nurturing-parent   \n",
      "268  1003     gpt4  bible             information-spreads-like-a-virus   \n",
      "269  1004       yi   orig  information-follows-individual-dispositions   \n",
      "\n",
      "                                                 Story  \\\n",
      "0    The strict father, Mr. Johnson, stood in the d...   \n",
      "1    In 'Alien', the Weyland-Yutani Corporation is ...   \n",
      "2    The nurturing parent frame is characterized by...   \n",
      "3    Sure! Here's an example from the movie 'Her': ...   \n",
      "4    Original Story:  As the sun set over the horiz...   \n",
      "..                                                 ...   \n",
      "265  As the sun began to rise over the sleepy town ...   \n",
      "266  In the heart of the bustling metropolis, whisp...   \n",
      "267  1 Thessalonians 2:7-8 (NIV)  \"But we were gent...   \n",
      "268  A fitting passage from the Bible that evokes t...   \n",
      "269  In a bustling city, there lived a curious libr...   \n",
      "\n",
      "                                            StorySanit  AnnotationScore  \\\n",
      "0    The father, Mr. Johnson, stood in the doorway ...                1   \n",
      "1                                                  NaN                0   \n",
      "2    Mrs. Johnson had always been a loving parent t...                1   \n",
      "3                                                  NaN                0   \n",
      "4                                                  NaN                1   \n",
      "..                                                 ...              ...   \n",
      "265                                                NaN                1   \n",
      "266                                                NaN                1   \n",
      "267                                                NaN                1   \n",
      "268                                                NaN                0   \n",
      "269                                                NaN                0   \n",
      "\n",
      "    Hallucination  \n",
      "0               -  \n",
      "1               0  \n",
      "2               -  \n",
      "3               0  \n",
      "4               -  \n",
      "..            ...  \n",
      "265             -  \n",
      "266             -  \n",
      "267             0  \n",
      "268             0  \n",
      "269             -  \n",
      "\n",
      "[270 rows x 8 columns]>\n",
      "#############################################\n",
      "df2 shape:  <bound method NDFrame.head of       ID   Model Source             Frame  \\\n",
      "0    0.0  llama2   orig     strict-father   \n",
      "1    2.0  llama2  scifi     strict-father   \n",
      "2    3.0  llama2   orig  nurturing-parent   \n",
      "3    5.0  llama2  scifi  nurturing-parent   \n",
      "4    6.0  llama2   orig       us-vs.-them   \n",
      "..   ...     ...    ...               ...   \n",
      "321  NaN     NaN    NaN               NaN   \n",
      "322  NaN     NaN    NaN               NaN   \n",
      "323  NaN     NaN    NaN               NaN   \n",
      "324  NaN     NaN    NaN               NaN   \n",
      "325  NaN     NaN    NaN               NaN   \n",
      "\n",
      "                                                 Story  \\\n",
      "0    The strict father, Mr. Johnson, stood in the d...   \n",
      "1    In 'Alien', the Weyland-Yutani Corporation is ...   \n",
      "2    The nurturing parent frame is characterized by...   \n",
      "3    Sure! Here's an example from the movie 'Her': ...   \n",
      "4    Original Story:  As the sun set over the horiz...   \n",
      "..                                                 ...   \n",
      "321                                                NaN   \n",
      "322                                                NaN   \n",
      "323                                                NaN   \n",
      "324                                                NaN   \n",
      "325                                                NaN   \n",
      "\n",
      "                                            StorySanit  AnnotationScore  \\\n",
      "0    The father, Mr. Johnson, stood in the doorway ...              1.0   \n",
      "1                                                  NaN              0.0   \n",
      "2    Mrs. Johnson had always been a loving parent t...              1.0   \n",
      "3                                                  NaN              0.0   \n",
      "4                                                  NaN              1.0   \n",
      "..                                                 ...              ...   \n",
      "321                                                NaN              NaN   \n",
      "322                                                NaN              NaN   \n",
      "323                                                NaN              NaN   \n",
      "324                                                NaN              NaN   \n",
      "325                                                NaN              NaN   \n",
      "\n",
      "    Hallucination                                              story  \\\n",
      "0               -                                                NaN   \n",
      "1               0                                                NaN   \n",
      "2               -                                                NaN   \n",
      "3               0                                                NaN   \n",
      "4               -                                                NaN   \n",
      "..            ...                                                ...   \n",
      "321           NaN  Jesus said this: “A new commandment I give to ...   \n",
      "322           NaN  The story should be set in a busy city interse...   \n",
      "323           NaN  \"And if one member suffers, all suffer; if one...   \n",
      "324           NaN  In the sci-fi movie 'Interstellar', humanity f...   \n",
      "325           NaN  1 Corinthians 12:12-14: 'Just as a body, thoug...   \n",
      "\n",
      "                     frame_label         source_notes humdoubt L3_SF L3_NP  \\\n",
      "0                            NaN                  NaN      NaN   NaN   NaN   \n",
      "1                            NaN                  NaN      NaN   NaN   NaN   \n",
      "2                            NaN                  NaN      NaN   NaN   NaN   \n",
      "3                            NaN                  NaN      NaN   NaN   NaN   \n",
      "4                            NaN                  NaN      NaN   NaN   NaN   \n",
      "..                           ...                  ...      ...   ...   ...   \n",
      "321  we-are-all-in-this-together        mistral-bible    False   NaN   NaN   \n",
      "322  we-are-all-in-this-together         mistral-orig    False   NaN   NaN   \n",
      "323  we-are-all-in-this-together  llama2-bible (halu)    False   NaN   NaN   \n",
      "324  we-are-all-in-this-together           gpt4-scifi    False   NaN   NaN   \n",
      "325  we-are-all-in-this-together           gpt4-bible    False  None  None   \n",
      "\n",
      "    L3_top5 L3s_SF L3s_NP L2_SF L2_NP  \n",
      "0       NaN    NaN    NaN   NaN   NaN  \n",
      "1       NaN    NaN    NaN   NaN   NaN  \n",
      "2       NaN    NaN    NaN   NaN   NaN  \n",
      "3       NaN    NaN    NaN   NaN   NaN  \n",
      "4       NaN    NaN    NaN   NaN   NaN  \n",
      "..      ...    ...    ...   ...   ...  \n",
      "321     NaN    NaN    NaN   NaN   NaN  \n",
      "322     NaN    NaN    NaN   NaN   NaN  \n",
      "323     NaN    NaN    NaN   NaN   NaN  \n",
      "324     NaN    NaN    NaN   NaN   NaN  \n",
      "325    None   None   None  None  None  \n",
      "\n",
      "[326 rows x 19 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Notebook to do a zeroshot analysis on our big-dataset of frames and say % each matches SF along with confidence %\n",
    "# (This new dataset will later be probed)\n",
    "\n",
    "# pip install -U pandas together\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "df2 = pd.read_csv(\"../1-Generation-and-Dataset/data-generated-stories-frames-202505.csv\")\n",
    "\n",
    "for ix, row in df2.sort_values(by='Frame', ascending=True).iterrows():\n",
    "    item = {}\n",
    "    item['story'] = row['StorySanit'] if row['StorySanit'] else row['Story']  # if has sanitized version, use that one\n",
    "    item['frame_label'] = row['Frame']  # already filtered to only score=1s, btw\n",
    "    item['source_notes'] = row['Model'] + '-' + row['Source'] + (' (halu)' if row['Hallucination']=='1' else '')\n",
    "    item['humdoubt'] = row['AnnotationScore'] != 1\n",
    "    mask = df2['Story'].str.contains(re.escape(item['story']))\n",
    "    item['L3_SF'] = row['L3_SF'] if 'L3_SF' in row else None    \n",
    "    item['L3_NP'] = row['L3_NP'] if 'L3_NP' in row else None\n",
    "    item['L3_top5'] = row['L3_top5'] if 'L3_top5' in row else None  \n",
    "    item['L3s_SF'] = row['L3s_SF'] if 'L3s_SF' in row else None\n",
    "    item['L3s_NP'] = row['L3s_NP'] if 'L3s_NP' in row else None\n",
    "    item['L2_SF'] = row['L2_SF'] if 'L2_SF' in row else None    \n",
    "    item['L2_NP'] = row['L2_NP'] if 'L2_NP' in row else None\n",
    "    df2 = pd.concat([df2, pd.DataFrame([item])], ignore_index=True)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from together import Together \n",
    "client = Together(api_key=\"68d210401de822340a017cbfa0486b12a3d14a12cc7f7c7c247844fc51edbbd9\")      \n",
    "\n",
    "df2['L3_SF_'] = np.nan\n",
    "df2['L3_NP_'] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290 strict-father → "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_986/1756133261.py:66: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '34' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df2.loc[ix, 'L3_NP_'] = resp_text.strip().replace('%', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan → 85.0 34\n",
      "323 we-are-all-in-this-together → nan nan → 12.0 34\n",
      "279 nurturing-parent → nan nan → 12.0 85\n",
      "312 we-are-all-in-this-together → nan nan → 14.0 34\n",
      "280 nurturing-parent → nan nan → 0.0 82\n",
      "273 nurturing-parent → nan nan → 0.0 80\n",
      "271 nurturing-parent → nan nan → 12.0 72\n",
      "284 nurturing-parent → nan nan → 80.0 80\n",
      "304 strict-father → nan nan → 85.0 75\n",
      "325 we-are-all-in-this-together → None None → 23.0 34\n",
      "317 we-are-all-in-this-together → nan nan → 34.0 63\n",
      "305 strict-father → nan nan → 85.0 75\n",
      "274 nurturing-parent → nan nan → 12.0 82\n",
      "283 nurturing-parent → nan nan → 0.0 85\n",
      "318 we-are-all-in-this-together → nan nan → 23.0 63\n",
      "311 we-are-all-in-this-together → nan nan → 34.0 63\n",
      "270 nurturing-parent → nan nan → 0.0 85\n",
      "287 nurturing-parent → nan nan → 34.0 85\n",
      "292 strict-father → nan nan → 34.0 34\n",
      "288 nurturing-parent → nan nan → 12.0 63\n",
      "302 strict-father → nan nan → 85.0 34\n",
      "285 nurturing-parent → nan nan → 63.0 63\n",
      "316 we-are-all-in-this-together → nan nan → 34.0 63\n",
      "275 nurturing-parent → nan nan → 0.0 85\n",
      "303 strict-father → nan nan → 85.0 63\n",
      "308 we-are-all-in-this-together → nan nan → 14.0 0\n",
      "281 nurturing-parent → nan nan → 34.0 64\n",
      "286 nurturing-parent → "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     43\u001b[39m prompt = \u001b[33m'\u001b[39m\u001b[33mWhat percentage does the following text evoke the \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstrict father\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m frame? (Please give just the percentage with no additional words)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     44\u001b[39m prompt += \u001b[33m'\u001b[39m\u001b[33m``\u001b[39m\u001b[33m'\u001b[39m + row[\u001b[33m'\u001b[39m\u001b[33mstory\u001b[39m\u001b[33m'\u001b[39m] + \u001b[33m'\u001b[39m\u001b[33m``\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta-llama/Llama-3-8b-chat-hf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# \"meta-llama/Llama-3-8b-chat-hf\",  #\"meta-llama/Llama-3-70b-chat-hf\"\u001b[39;49;00m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# hmmm\u001b[39;49;00m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\n\u001b[32m     50\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m resp_text = response.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m     52\u001b[39m resp_text = resp_text.lower().replace(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m).replace(\u001b[33m\"\u001b[39m\u001b[33msure, here are my estimates:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).replace(\u001b[33m\"\u001b[39m\u001b[33msure! here are my estimates:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).replace(\u001b[33m\"\u001b[39m\u001b[33mhere are the percentages:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).replace(\u001b[33m\"\u001b[39m\u001b[33m frame\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).replace(\u001b[33m\"\u001b[39m\u001b[33m* \u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).replace(\u001b[33m\"\u001b[39m\u001b[33m level\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).replace(\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m).replace(\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/together/resources/chat/completions.py:141\u001b[39m, in \u001b[36mChatCompletions.create\u001b[39m\u001b[34m(self, messages, model, max_tokens, stop, temperature, top_p, top_k, repetition_penalty, presence_penalty, frequency_penalty, min_p, logit_bias, seed, stream, logprobs, echo, n, safety_model, response_format, tools, tool_choice, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m requestor = api_requestor.APIRequestor(\n\u001b[32m    113\u001b[39m     client=\u001b[38;5;28mself\u001b[39m._client,\n\u001b[32m    114\u001b[39m )\n\u001b[32m    116\u001b[39m parameter_payload = ChatCompletionRequest(\n\u001b[32m    117\u001b[39m     model=model,\n\u001b[32m    118\u001b[39m     messages=messages,\n\u001b[32m   (...)\u001b[39m\u001b[32m    138\u001b[39m     **kwargs,\n\u001b[32m    139\u001b[39m ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m response, _, _ = \u001b[43mrequestor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTogetherRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameter_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    151\u001b[39m     \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, TogetherResponse)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/together/abstract/api_requestor.py:242\u001b[39m, in \u001b[36mAPIRequestor.request\u001b[39m\u001b[34m(self, options, stream, remaining_retries, request_timeout)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    232\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    233\u001b[39m     options: TogetherRequest,\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m     \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    241\u001b[39m ]:\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest_raw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m     resp, got_stream = \u001b[38;5;28mself\u001b[39m._interpret_response(result, stream)\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m.api_key\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/together/abstract/api_requestor.py:545\u001b[39m, in \u001b[36mAPIRequestor.request_raw\u001b[39m\u001b[34m(self, options, remaining_retries, stream, request_timeout, absolute)\u001b[39m\n\u001b[32m    542\u001b[39m         result_headers = \u001b[38;5;28mdict\u001b[39m(result.headers) \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    544\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m                \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m                \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m                \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresult_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m status_code = result.status_code \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    554\u001b[39m result_headers = \u001b[38;5;28mdict\u001b[39m(result.headers) \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/together/abstract/api_requestor.py:192\u001b[39m, in \u001b[36mAPIRequestor._retry_request\u001b[39m\u001b[34m(self, options, remaining_retries, response_headers, stream, request_timeout)\u001b[39m\n\u001b[32m    188\u001b[39m (\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m time.sleep(timeout)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request_raw(\n\u001b[32m    195\u001b[39m     options=options,\n\u001b[32m    196\u001b[39m     stream=stream,\n\u001b[32m    197\u001b[39m     request_timeout=request_timeout,\n\u001b[32m    198\u001b[39m     remaining_retries=remaining,\n\u001b[32m    199\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for ix, row in df2.sort_values(by='story', ascending=True).iterrows():\n",
    "    #if row['humdoubt'] is True:\n",
    "        # already done!\n",
    "    #    continue\n",
    "\n",
    "    print(ix, row['frame_label'], end=\" → \")\n",
    "\n",
    "    # TODO: REDO LLAMA3-70B once more with `EVOKE`` INSTEAD OF `INVOKE` to see if results change? -- ALSO REDO 'TEXT' INSTEAD OF 'STORY'\n",
    "\n",
    "    # # L2: SF percent\n",
    "    # prompt = 'What percentage does the following text invoke the \"strict father\" frame? (Please give just the percentage with no additional words)\\n'\n",
    "    # prompt += '``' + row['story'] + '``'\n",
    "    # response = client.chat.completions.create(\n",
    "    #     model=\"meta-llama/LLama-2-7b-chat-hf\",  \n",
    "    #     messages=[{\"role\": \"user\",  \"content\": prompt}],\n",
    "    #     temperature=0,   # hmmm\n",
    "    #     max_tokens=50\n",
    "    # )\n",
    "    # resp_text = response.choices[0].message.content\n",
    "    # resp_text = resp_text.lower().replace(\"\\n\", \" \").strip()  \n",
    "    # pattern = r'(?:strict father.*?(\\d+)%|(\\d+)%$)'\n",
    "    # match = re.search(pattern, resp_text)\n",
    "    # df2.loc[ix, 'L2_SF'] = int(match.group(1) or match.group(2)) if match else resp_text.replace('%', '')\n",
    "\n",
    "    # #L2: NP percent\n",
    "    # prompt = 'What percentage does the following text invoke the \"nuturing parent\" frame? (Please give just the percentage with no additional words)\\n'\n",
    "    # prompt += '``' + row['story'] + '``'\n",
    "    # response = client.chat.completions.create(\n",
    "    #     model=\"meta-llama/LLama-2-7b-chat-hf\",  \n",
    "    #     messages=[{\"role\": \"user\",  \"content\": prompt}],\n",
    "    #     temperature=0,   # hmmm\n",
    "    #     max_tokens=50\n",
    "    # )\n",
    "    # resp_text = response.choices[0].message.content\n",
    "    # resp_text = resp_text.lower().replace(\"\\n\", \" \").strip()  \n",
    "    # pattern = r'(?:nurturing parent.*?(\\d+)%|(\\d+)%$)'\n",
    "    # match = re.search(pattern, resp_text)\n",
    "    # df2.loc[ix, 'L2_NP'] = int(match.group(1) or match.group(2)) if match else resp_text\n",
    "\n",
    "    # L3: SF percent\n",
    "    prompt = 'What percentage does the following text evoke the \"strict father\" frame? (Please give just the percentage with no additional words)\\n'\n",
    "    prompt += '``' + row['story'] + '``'\n",
    "    response = client.chat.completions.create(\n",
    "        model= \"meta-llama/Llama-3-8b-chat-hf\",   # \"meta-llama/Llama-3-8b-chat-hf\",  #\"meta-llama/Llama-3-70b-chat-hf\"\n",
    "        messages=[{\"role\": \"user\",  \"content\": prompt}],\n",
    "        temperature=0,   # hmmm\n",
    "        max_tokens=50\n",
    "    )\n",
    "    resp_text = response.choices[0].message.content\n",
    "    resp_text = resp_text.lower().replace(\"\\n\", \" \").replace(\"sure, here are my estimates:\", \"\").replace(\"sure! here are my estimates:\", \"\").replace(\"here are the percentages:\", \"\").replace(\" frame\", \"\").replace(\"* \", \"\").replace(\" level\", \"\").replace('\"', '').replace(',', '')\n",
    "    df2.loc[ix, 'L3_SF_'] = int(resp_text.strip().replace('%', ''))\n",
    "\n",
    "    # # L3: NP percent\n",
    "    prompt = 'What percentage does the following text evoke the \"nuturing parent\" frame? (Please give just the percentage with no additional words)\\n'\n",
    "    prompt += '``' + row['story'] + '``'\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3-8b-chat-hf\",   # \"meta-llama/Llama-3-8b-chat-hf\",  # \"meta-llama/Llama-3-70b-chat-hf\"\n",
    "        messages=[{\"role\": \"user\",  \"content\": prompt}],\n",
    "        temperature=0,   # hmmm\n",
    "        max_tokens=50\n",
    "    )\n",
    "    resp_text = response.choices[0].message.content\n",
    "    resp_text = resp_text.lower().replace(\"\\n\", \" \").replace(\"sure, here are my estimates:\", \"\").replace(\"sure! here are my estimates:\", \"\").replace(\"here are the percentages:\", \"\").replace(\" frame\", \"\").replace(\"* \", \"\").replace(\" level\", \"\").replace('\"', '').replace(',', '')\n",
    "    df2.loc[ix, 'L3_NP_'] = resp_text.strip().replace('%', '')\n",
    "\n",
    "    #    # L3: SF percent\n",
    "    # prompt = 'What percentage does the following text invoke the \"strict father\" frame? (Please give just the percentage with no additional words)\\n'\n",
    "    # prompt += '``' + row['story'] + '``'\n",
    "    # response = client.chat.completions.create(\n",
    "    #     model=\"meta-llama/Llama-3-70b-chat-hf\",   # \n",
    "    #     messages=[{\"role\": \"user\",  \"content\": prompt}],\n",
    "    #     temperature=0,   # hmmm\n",
    "    #     max_tokens=50\n",
    "    # )\n",
    "    # resp_text = response.choices[0].message.content\n",
    "    # resp_text = resp_text.lower().replace(\"\\n\", \" \").replace(\"sure, here are my estimates:\", \"\").replace(\"sure! here are my estimates:\", \"\").replace(\"here are the percentages:\", \"\").replace(\" frame\", \"\").replace(\"* \", \"\").replace(\" level\", \"\").replace('\"', '').replace(',', '')\n",
    "    # df2.loc[ix, 'L3_SF'] = resp_text.strip().replace('%', '')\n",
    "\n",
    "    # # # L3: NP percent\n",
    "    # prompt = 'What percentage does the following text invoke the \"nuturing parent\" frame? (Please give just the percentage with no additional words)\\n'\n",
    "    # prompt += '``' + row['story'] + '``'\n",
    "    # response = client.chat.completions.create(\n",
    "    #     model=\"meta-llama/Llama-3-70b-chat-hf\",   # \n",
    "    #     messages=[{\"role\": \"user\",  \"content\": prompt}],\n",
    "    #     temperature=0,   # hmmm\n",
    "    #     max_tokens=50\n",
    "    # )\n",
    "    # resp_text = response.choices[0].message.content\n",
    "    # resp_text = resp_text.lower().replace(\"\\n\", \" \").replace(\"sure, here are my estimates:\", \"\").replace(\"sure! here are my estimates:\", \"\").replace(\"here are the percentages:\", \"\").replace(\" frame\", \"\").replace(\"* \", \"\").replace(\" level\", \"\").replace('\"', '').replace(',', '')\n",
    "    # df2.loc[ix, 'L3_NP'] = resp_text.strip().replace('%', '')\n",
    "\n",
    "    # # L3: top five frames\n",
    "    # prompt = 'Can you tell me which major cognitive frames are invoked by the following text? (Please keep your answer strictly short and name max 5 frames with no explanation)\\n'\n",
    "    # prompt += '``' + row['story'] + '``'\n",
    "    # response = client.chat.completions.create(\n",
    "    #     model=\"meta-llama/Llama-3-70b-chat-hf\",   # \"meta-llama/Llama-3-8b-chat-hf\",\n",
    "    #     messages=[{\"role\": \"user\",  \"content\": prompt}],\n",
    "    #     temperature=0,   # hmmm\n",
    "    #     max_tokens=50\n",
    "    # )\n",
    "    # resp_text = response.choices[0].message.content\n",
    "    # resp_text = resp_text.replace(\"\\n\", \" \").replace(\"The frame invoked by\", \"\").replace(\"The answer is:\", \"\").replace(\"Sure!\", \"\").replace(\" The story invokes the\", \"\")\n",
    "    # resp_text = resp_text.replace(\"Here are the major cognitive frames invoked by the story:\", \"\").replace(\"Here are 5 major cognitive frames invoked by the story:\", \"\").replace(\"Here are the 5 major cognitive frames invoked by the passage:\", \"\").replace(\"Here are the 5 major cognitive frames invoked by the story:\", \"\")\n",
    "    # resp_text = resp_text.replace(\"1. \", \"\").replace(\" 2.\", \",\").replace(\" 3.\", \",\").replace(\" 4.\", \",\").replace(\" 5.\", \",\")\n",
    "    # df2.loc[ix, 'L3_top5'] = resp_text.strip()\n",
    "\n",
    "    print(df2.loc[ix, 'L3_SF'], df2.loc[ix, 'L3_NP'], \"→\", df2.loc[ix, 'L3_SF_'], df2.loc[ix, 'L3_NP_'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
